* anti-replay TLS Ã  activer ?
* voir ce qu'il faut faire quand le build d'un packet Ã©choue pour les trames,
  peut-Ãªtre ne les insÃ©rer dans le paquet que si correctement buildÃ© (le paquet)
  -> le MUX fait du retry opportuniste, ce serait plutÃ´t Ã  la quic-conn de le faire
  -> voir les URLs des gros fichiers sur haproxy.org pour tester

* voir si le paquet Version Negotiation doit Ãªtre toujours Ã©mis (ou que pour une connection Ã©tablie)

DDOS -> * drop des paquets 0RTT
 -> voir Ã  activer l'option SSL_OP_NO_ANTI_REPLAY
 si problÃ©matique peut Ãªtre annoncer le non-support de 0RTT avec QUIC

* limiter le boulot pour quic_lstnr_dghdlr() (nombre de dgrams Ã  traiter ?)
rÃ©utiliser le max-poll-event ou avoir un settings indÃ©pendant

OPTIM * diminuer le nombre de tasklet_wakeup nÃ©cessaire au traitement d'un paquet

DDOS -> * Retry : revoir la gÃ©nÃ©ration du token
  -> gÃ©nÃ©rer une clÃ© random au dÃ©marrage, changer la clÃ© toutes les 10s (invalider la clÃ© d'avant ou garder la clÃ© prÃ©cÃ©dente)
  -> concatÃ©nÃ©r avec l'adresse IP du client + le DCID
  -> avoir une heuristique pour activer la validation du Retry
      probablement un compteur de connexions en cours d'activation (quic-conn avant handshake), possiblement configurÃ© via un settings global

AmÃ©lioration diagnostic QUIC
* support pcap + keylogfile
* support QLOG
  -> regarder intÃ©rÃªt du QLOG avec QVIS
* amÃ©lioration trace
  -> dÃ©finir des niveaux de traces
  -> rÃ©utiliser la quic_conn pour lockon sur les traces MUX
  -> avoir des traces H3

Tx transport quic-conn
* utiliser sendmsg/sendmmsg pour support envoi datagrammes >16ko
URGENT -> * gÃ©rer les cas d'erreur de sendto
  nÃ©cessite de gÃ©rer l'abonnemement des fds au poller via fd_cant_recv
  abonnement des quic_conn dans des listes par threads attachÃ©s aux receivers
  sur rÃ©veil de l'IO CB du receiver, lock ou detach de la list head par le
  premier thread rÃ©veillÃ©, et wakeup sur l'ensemble des listes
URGENT -> * revue des erreurs sur envoi trames
  trier les erreurs temporaires et les erreurs permanentes
  permettre au maxium Ã  la couche MUX de s'inscrire en cas d'erreur
  nÃ©cessite que la quic-conn rÃ©veille le MUX ensuite sur la condition (rÃ©ception ACK si congestion, allocation rÃ©ussie si erreur alloc, ...)
  en cas d'erreur permanente fermer la connexion voire le process


DDOS -> * Initial pkt len check : vÃ©rifier quelle soit bien checkÃ©e (1200 octets ?)

* la quic-conn peut Ãªtre conservÃ©e le temps de la durÃ©e de vie du MUX
  possiblement nettoyer des parties devenus inutile de la quic-conn

* fred a fait grave de la merde
encore une fois
problÃ¨me d'initialisation des adresses des connexions dans new_quic_cli_conn()
saddr utilisÃ© pour conn->dst -> doit Ãªtre conn->src
 et flag TO_SET -> FROM_SET
 et il faut rÃ©cupÃ©rer l'adresse destination pour la mettre dans conn->dst
doit fixer l'affichage dans les LOGS des adresses sur haproxy.org via PROXY_PROTOCOL

BUG -> * MUX problÃ¨me flag CS_EP_EOS
positionnÃ© trop tÃ´t, ce qui cause l'Ã©chec d'un POST avec HTTP 400 avec abortonclose
ne doit Ãªtre mis que lorsque un stream est sur le point d'Ãªtre fermÃ© Rx+Tx

BUG/CRASH -> MUX interrompu en cours de transfer
  0x000055555560ca60 in qcs_free (qcs=0x7ffff0049e90) at src/mux_quic.c:181
  181             BUG_ON(qcs->endp && !(qcs->endp->flags & CS_EP_ORPHAN));
implÃ©mentÃ© une logique similaire au mux-h2
envoi d'un GOAWAY ou release si aucun stream ouvert
laisser les streams ouverts se terminÃ©s de maniÃ¨re habituelle

STREAM
 -> erreur si rÃ©ception du meme offset avec des data diffÃ©rentes
   PROTOCOL_VIOLATION
rfc9000 2.2. Sending and Receiving Data
 The data at a given offset MUST NOT change if it is sent
 multiple times; an endpoint MAY treat receipt of different data at
 the same offset within a stream as a connection error of type
 PROTOCOL_VIOLATION.

NCBUF -> static inline pour les fonctions les plus utilisÃ©es

DISPATCH DES CONNEXIONS
 * sur rÃ©ception des premiers paquets Initial avec ODCID, s'assurer qu'on
 redispatch sur un thread valide pour le listener
 * sur allocation des nouveaux CID, assigner le CID sur le thread le moins
 chargÃ© plutÃ´t que le thread courant. Pour Ã©viter de locker sur crÃ©ation CID,
 probablement redispatcher le paquet vers le thread le moins chargÃ© et flagger
 le paquet comme devant donner lieu Ã  crÃ©ation de la quic_conn.

 Sur ce point, il serait possible de se passer de l'arbre des ODCID en assignant
 nous-mêmes le CID à partir de l'ODCID. On le hashe avec sa source, ça nous donne
 les 52 bits de poids fort du CID (si c'est bien du 64-bits), et on stocke l'ID
 du thread dans les 12 bits de poids faible. Comme ça on n'utilise plus que les
 CID finaux comme clé de stockage, et si on ne trouve pas un CID lors d'un lookup,
 alors on considère que c'est peut-être un ODCID, on applique le calcul ci-dessus
 (sans le thread id) et on fait un lookup_ge() dans l'arbre (les arbres?) de
 connexions. Si on en trouve un qui correspond, c'est gagné et on a en prime le
 thread ID associé. Donc on pourrait très bien stocker les CID finaux dans un
 arbre locké temporaire (équivalent des ODCID actuels) tant que le client n'a
 pas confirmé l'utilisation de notre CID, et ne les déplacer dans l'arbre du
 thread qu'une fois que le client a confirmé avoir adopté notre CID. Ainsi, on
 n'a jamais besoin de mémoriser l'ODCID, soit le client parle avec le CID et on
 a l'info en direct, soit il parle avec l'ODCID et on retrouve l'info.

CONNECTION_CLOSE DURING HANDSHAKE
cf RFC 9000 10.2.3.
https://datatracker.ietf.org/doc/html/rfc9000#section-10.2.3
  si envoi nÃ©cessaire d'un CONNECTION_CLOSE durant la handshake, il est parfois
  requis de gÃ©nÃ©rer la trame dans plusieurs paquets Ã  diffÃ©rents niveau
  d'encryption selon le contexte du client pour garantir qu'il pourra Ãªtre
  dÃ©chiffrÃ©. Ã€ l'heure actuelle cela n'est pas fait et le CONNECTION_CLOSE est
  envoyÃ© uniquement au niveau actuel de la struct quic_conn

RACE FIN DE HANDSHAKE et INIT MUX ?
vÃ©rifier s'il est possible de recevoir des STREAM ou autres trames destinÃ©s au
MUX avant son initialisation. Si oui, prÃ©voir de bufferiser ces trames ou
dÃ©sactiver temporairement la lecture du Rx buf, ou au pire dropper ces trames
en attente de rÃ©Ã©mission par le client.

MAINTIEN DE LA CONNEXION PAR LA QUIC-CONN
la quic-conn devrait tenter de maintenir la connexion explicitement ouverte
tant que le MUX reste actif. Pour cela il est nÃ©cessaire d'Ã©mettre des PING si
aucune communication en cours. Cependant il faut rester Ã©conome, possiblement
en envoyant un PING que sur expiration proche de l'idle timeout QUIC.

OPTIM -> Ã©mission RESET_STREAM par le MUX
chaque stream envoie son RESET_STREAM indÃ©pendamment Ã  la couche QUIC-CONN ce
qui cause l'envoi d'un datagramme par RESET_STREAM. Ã€ optimiser tout en gardant
la possibilitÃ© de savoir si un RESET_STREAM n'a pu Ãªtre envoyÃ© pour un stream.

FERMETURE CONNECTION MUX
amÃ©liorer l'envoi GOAWAY / CONNECTION_CLOSE
Ã  l'heure actuelle fermeture du MUX aprÃ¨s envoi du GOAWAY
* soit envoyer GOAWAY+CONNECTION_CLOSE si possible
* ou graceful shutdown avec envoi GOAWAY et maintien du MUX jusqu'Ã  envoi du CONNECTION_CLOSE

H3
* supporter les trailers HTTP/3

REFACTOR
* qc_dup_pkt_frms(): parsing des streams Ã  refactorer avec le recv ?
* utiliser ncbuf pour ACK ranges
* quic_conn_io_cb(): Ã©tendre la fonction pour pouvoir gÃ©rer plus de 2 encryption levels
  nÃ©cessaire notamment pour l'envoi de CONNECTION_CLOSE lors de la handshake
* faire de QPACK un application protocol layer au mÃªme niveau que H3
* identifier le code commun h3/hq-interop et le dÃ©placer dans la couche MUX
  par exemple h3_decode_qcs/hq_interop_decode_qcs vers qcc_decode_qcs
    h3_snd_buf/hq_interop_snd_buf vers qc_snd_buf

ARCHI REVIEW
* sur reception CONNECTION_CLOSE
  retourner sur qc_send_mux un code d'erreur
* avant envoi du MUX vÃ©rifier taille de window pour ne pas envoyer plus que nÃ©cessaire
* Ã©viter de faire du travail pour rien
  dÃ¨s qu'un datagramme est prÃªt par qc_prep_app_pkts, l'Ã©mettre via qc_send_ppkts
  plutot que gÃ©nÃ©rer une liste de datagrammes et tout perdre si erreur sur le premier envoi
* Nouvelle API d'envoi
  -> appel de la quic-conn par le MUX sur chaque stream
  -> remplissage du buffer d'envoi par la quic-conn
  -> envoi du buffer lorsque datagramme rempli
  -> conversation entre chaque envoi

GESTION DE LA MIGRATION DANS LE CAS DES FD PAR CONNEXION
 Lorsqu'on aura un FD par connexion, le trafic reçu par les FD se répartira
 comme suit:
   - FD du receiver: quelques occasionnels datagrammes d'une nouvelle connexion
     auront déjà été mis en queue par le kernel s'ils sont reçus avant que
     le bind()+connect() ne se fasse, donc ils seront reçus par ce biais et
     il faudra maintenir le dispatch
   - FD de la nouvelle connexion: quelques occasionnels datagrammes destinés
     à n'importe quelle autre connexion arriveront entre le bind() et le
     connect(). La plupart du temps il s'agira de datagrammes non mappés sur
     une connexion, ou appartenant à une connexion non encore finalisée.
   - dans les deux cas il faudra faire le redispatch vers la bonne queue
     (c'est à dire qu'on retombe dans le cas actuel pour une infime part des
     datagrammes reçus).
   - toutefois en cas de migration d'IP et/ou de port, la connexion ne matchera
     plus les paquets et le trafic arrivera à nouveau par le receiver global.
     Il faudra alors être en mesure de flaguer la connexion pour dire qu'elle
     est en cours de migration, de sorte que son I/O handler, dès qu'il aura
     besoin d'émettre, bute la connexion en cours et la recrée vers la nouvelle
     destination (soit avec le même FD soit avec un autre selon ce qui est
     possible). Au moins ça se fera depuis le bon thread et on n'aura pas
     d'autre problème que ça.
 => il faut prévoir qu'il y aura toujours une toute petite part de trafic mal
    aiguillé et donc qu'il faut impérativement qu'on puisse le redispatcher
    en interne comme on le fait déjà.
