* anti-replay TLS Ã  activer ?
* voir ce qu'il faut faire quand le build d'un packet Ã©choue pour les trames,
  peut-Ãªtre ne les insÃ©rer dans le paquet que si correctement buildÃ© (le paquet)
  -> le MUX fait du retry opportuniste, ce serait plutÃ´t Ã  la quic-conn de le faire
  -> voir les URLs des gros fichiers sur haproxy.org pour tester

* voir si le paquet Version Negotiation doit Ãªtre toujours Ã©mis (ou que pour une connection Ã©tablie)

DDOS -> * drop des paquets 0RTT
 -> voir Ã  activer l'option SSL_OP_NO_ANTI_REPLAY
 si problÃ©matique peut Ãªtre annoncer le non-support de 0RTT avec QUIC

OPTIM * diminuer le nombre de tasklet_wakeup nÃ©cessaire au traitement d'un paquet

AmÃ©lioration diagnostic QUIC
* support pcap + keylogfile
* support QLOG
  -> regarder intÃ©rÃªt du QLOG avec QVIS
* amÃ©lioration trace
  -> dÃ©finir des niveaux de traces
  -> rÃ©utiliser la quic_conn pour lockon sur les traces MUX

Tx transport quic-conn
* utiliser sendmsg/sendmmsg pour support envoi datagrammes >16ko
URGENT -> * gÃ©rer les cas d'erreur de sendto
  nÃ©cessite de gÃ©rer l'abonnemement des fds au poller via fd_cant_recv
  abonnement des quic_conn dans des listes par threads attachÃ©s aux receivers
  sur rÃ©veil de l'IO CB du receiver, lock ou detach de la list head par le
  premier thread rÃ©veillÃ©, et wakeup sur l'ensemble des listes
URGENT -> * revue des erreurs sur envoi trames
  trier les erreurs temporaires et les erreurs permanentes
  permettre au maxium Ã  la couche MUX de s'inscrire en cas d'erreur
  nÃ©cessite que la quic-conn rÃ©veille le MUX ensuite sur la condition (rÃ©ception ACK si congestion, allocation rÃ©ussie si erreur alloc, ...)
  en cas d'erreur permanente fermer la connexion voire le process

* la quic-conn peut Ãªtre conservÃ©e le temps de la durÃ©e de vie du MUX
  possiblement nettoyer des parties devenus inutile de la quic-conn

DISPATCH DES CONNEXIONS
 * sur allocation des nouveaux CID, assigner le CID sur le thread le moins
 chargÃ© plutÃ´t que le thread courant. Pour Ã©viter de locker sur crÃ©ation CID,
 probablement redispatcher le paquet vers le thread le moins chargÃ© et flagger
 le paquet comme devant donner lieu Ã  crÃ©ation de la quic_conn.

 Sur ce point, il serait possible de se passer de l'arbre des ODCID en assignant
 nous-mêmes le CID à partir de l'ODCID. On le hashe avec sa source, ça nous donne
 les 52 bits de poids fort du CID (si c'est bien du 64-bits), et on stocke l'ID
 du thread dans les 12 bits de poids faible. Comme ça on n'utilise plus que les
 CID finaux comme clé de stockage, et si on ne trouve pas un CID lors d'un lookup,
 alors on considère que c'est peut-être un ODCID, on applique le calcul ci-dessus
 (sans le thread id) et on fait un lookup_ge() dans l'arbre (les arbres?) de
 connexions. Si on en trouve un qui correspond, c'est gagné et on a en prime le
 thread ID associé. Donc on pourrait très bien stocker les CID finaux dans un
 arbre locké temporaire (équivalent des ODCID actuels) tant que le client n'a
 pas confirmé l'utilisation de notre CID, et ne les déplacer dans l'arbre du
 thread qu'une fois que le client a confirmé avoir adopté notre CID. Ainsi, on
 n'a jamais besoin de mémoriser l'ODCID, soit le client parle avec le CID et on
 a l'info en direct, soit il parle avec l'ODCID et on retrouve l'info.

CONNECTION_CLOSE DURING HANDSHAKE
cf RFC 9000 10.2.3.
https://datatracker.ietf.org/doc/html/rfc9000#section-10.2.3
  si envoi nÃ©cessaire d'un CONNECTION_CLOSE durant la handshake, il est parfois
  requis de gÃ©nÃ©rer la trame dans plusieurs paquets Ã  diffÃ©rents niveau
  d'encryption selon le contexte du client pour garantir qu'il pourra Ãªtre
  dÃ©chiffrÃ©. Ã€ l'heure actuelle cela n'est pas fait et le CONNECTION_CLOSE est
  envoyÃ© uniquement au niveau actuel de la struct quic_conn

RACE FIN DE HANDSHAKE et INIT MUX ?
vÃ©rifier s'il est possible de recevoir des STREAM ou autres trames destinÃ©s au
MUX avant son initialisation. Si oui, prÃ©voir de bufferiser ces trames ou
dÃ©sactiver temporairement la lecture du Rx buf, ou au pire dropper ces trames
en attente de rÃ©Ã©mission par le client.

MAINTIEN DE LA CONNEXION PAR LA QUIC-CONN
la quic-conn devrait tenter de maintenir la connexion explicitement ouverte
tant que le MUX reste actif. Pour cela il est nÃ©cessaire d'Ã©mettre des PING si
aucune communication en cours. Cependant il faut rester Ã©conome, possiblement
en envoyant un PING que sur expiration proche de l'idle timeout QUIC.

OPTIM -> Ã©mission RESET_STREAM par le MUX
chaque stream envoie son RESET_STREAM indÃ©pendamment Ã  la couche QUIC-CONN ce
qui cause l'envoi d'un datagramme par RESET_STREAM. Ã€ optimiser tout en gardant
la possibilitÃ© de savoir si un RESET_STREAM n'a pu Ãªtre envoyÃ© pour un stream.

FERMETURE CONNECTION MUX
amÃ©liorer l'envoi GOAWAY / CONNECTION_CLOSE

H3
* supporter les trailers HTTP/3

REFACTOR
* qc_dup_pkt_frms(): parsing des streams Ã  refactorer avec le recv ?
* utiliser ncbuf pour ACK ranges
* quic_conn_io_cb(): Ã©tendre la fonction pour pouvoir gÃ©rer plus de 2 encryption levels
  nÃ©cessaire notamment pour l'envoi de CONNECTION_CLOSE lors de la handshake
* faire de QPACK un application protocol layer au mÃªme niveau que H3
* identifier le code commun h3/hq-interop et le dÃ©placer dans la couche MUX
  par exemple h3_decode_qcs/hq_interop_decode_qcs vers qcc_decode_qcs
    h3_snd_buf/hq_interop_snd_buf vers qc_snd_buf

ARCHI REVIEW
* sur reception CONNECTION_CLOSE
  retourner sur qc_send_mux un code d'erreur
* avant envoi du MUX vÃ©rifier taille de window pour ne pas envoyer plus que nÃ©cessaire
* Ã©viter de faire du travail pour rien
  dÃ¨s qu'un datagramme est prÃªt par qc_prep_app_pkts, l'Ã©mettre via qc_send_ppkts
  plutot que gÃ©nÃ©rer une liste de datagrammes et tout perdre si erreur sur le premier envoi
* Nouvelle API d'envoi
  -> appel de la quic-conn par le MUX sur chaque stream
  -> remplissage du buffer d'envoi par la quic-conn
  -> envoi du buffer lorsque datagramme rempli
  -> conversation entre chaque envoi
=> MINOR

GESTION DE LA MIGRATION DANS LE CAS DES FD PAR CONNEXION
 Lorsqu'on aura un FD par connexion, le trafic reçu par les FD se répartira
 comme suit:
   - toutefois en cas de migration d'IP et/ou de port, la connexion ne matchera
     plus les paquets et le trafic arrivera à nouveau par le receiver global.
     Il faudra alors être en mesure de flaguer la connexion pour dire qu'elle
     est en cours de migration, de sorte que son I/O handler, dès qu'il aura
     besoin d'émettre, bute la connexion en cours et la recrée vers la nouvelle
     destination (soit avec le même FD soit avec un autre selon ce qui est
     possible). Au moins ça se fera depuis le bon thread et on n'aura pas
     d'autre problème que ça.
 => il faut prévoir qu'il y aura toujours une toute petite part de trafic mal
    aiguillé et donc qu'il faut impérativement qu'on puisse le redispatcher
    en interne comme on le fait déjà.

RECV DATAGRAM / RXBUF (QUIC_RECEIVER_BUF)
* ne pas faire d'APPEND systématique après usage d'un rxbuf
  plutot faire un INSERT en tête de liste, sauf si taille insuffisante pour
  prochains recv
  -> possiblement dépiler un nouveau rxbuf si multiple datagrammes lus
* remplacer le fake datagram par un champ de la structure rxbuf
  cela économiserait l'allocation d'un quic_dgram
* faire un realign_if_empty, probablement avant de tester l'espace contigue
* remplacer l'io-cb par une tasklet
  obligatoire pour supporter le polling en mode edge-triggered
  wakeup de la tasklet si on a atteint le nombre max de dgrams à lire avant de bloquer
  un bon compromis serait de lire maximum 2 datagrammes directement dans l'io-cb
  si 2e datagramme lu, faire un tasklet_wakeup afin de limiter le travail dans l'io-cb
